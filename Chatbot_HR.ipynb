{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNixP7/N/nCBBBprurb8vAs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanshika1302/HRChatBot/blob/main/Chatbot_HR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install langchain openai chromadb pandas"
      ],
      "metadata": {
        "id": "nVREyg6pu__t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZanT7L8huxbg"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6505c204"
      },
      "source": [
        "!pip install -U langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "GPABunWVwHUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "Company HR Policy:\n",
        "\n",
        "1. Employees are entitled to 20 days of paid annual leave.\n",
        "2. Sick leave requires a medical certificate if longer than 2 days.\n",
        "3. Work from home is allowed 2 days per week with manager approval.\n",
        "4. Payroll is processed on the 25th of each month.\n",
        "5. Overtime will be compensated as per company guidelines.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(sample_text)"
      ],
      "metadata": {
        "id": "vrg7SyrMxXzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"hr_policy.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(chunks[0].page_content)"
      ],
      "metadata": {
        "id": "NV4VI1ilxlN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d1efe6"
      },
      "source": [
        "!pip install chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # will ask you to paste your token"
      ],
      "metadata": {
        "id": "t22zPKuX1GDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub transformers"
      ],
      "metadata": {
        "id": "YMacWNYnzjsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers accelerate scipy"
      ],
      "metadata": {
        "id": "QwcvoPUd245R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Read the chunks we created earlier (if you used the splitter, use that list 'chunks')\n",
        "# If you followed earlier steps you have `chunks` that are Document objects with .page_content\n",
        "# For safety, we'll create a simple chunks list from the file if not present:\n",
        "\n",
        "try:\n",
        "    texts = [c.page_content for c in chunks]   # if you already created `chunks` via LangChain\n",
        "except NameError:\n",
        "    with open(\"hr_policy.txt\", \"r\") as f:\n",
        "        txt = f.read()\n",
        "    # small manual split (for tiny doc)\n",
        "    texts = [txt]\n",
        "\n",
        "# 2) Load a sentence-transformer model for embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")   # small, fast, good for semantics\n",
        "\n",
        "# 3) Compute embeddings for each chunk\n",
        "embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
        "print(\"Loaded\", len(texts), \"chunks and computed embeddings; embedding_dim =\", embeddings.shape[1])"
      ],
      "metadata": {
        "id": "ppcPDtF05tM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def retrieve_top_k(question, k=2):\n",
        "    \"\"\"\n",
        "    - embeds the question\n",
        "    - computes cosine similarity with document chunk embeddings\n",
        "    - returns top-k chunk texts (most similar)\n",
        "    \"\"\"\n",
        "    q_emb = embed_model.encode([question], convert_to_numpy=True)  # shape (1, dim)\n",
        "    # compute cosine distances between question and all chunks (cdist gives distances; 1 - similarity)\n",
        "    distances = cdist(q_emb, embeddings, metric=\"cosine\")[0]       # shape (n_chunks,)\n",
        "    # smaller distance => more similar. get k smallest distances\n",
        "    topk_idx = np.argsort(distances)[:k]\n",
        "    topk_texts = [texts[i] for i in topk_idx]\n",
        "    topk_scores = [1 - float(distances[i]) for i in topk_idx]     # convert to similarity score\n",
        "    return topk_texts, topk_scores"
      ],
      "metadata": {
        "id": "h0yibVnv7Xf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# load the small Flan-T5 text2text model (runs in Colab)\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# create a text2text pipeline for generation\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0 if __import__(\"torch\").cuda.is_available() else -1)\n",
        "\n",
        "def answer_question(question, k=2, max_length=256):\n",
        "    # 1. retrieve relevant context\n",
        "    top_texts, scores = retrieve_top_k(question, k=k)\n",
        "    # 2. build prompt: provide context + question (clear instruction)\n",
        "    context_text = \"\\n\".join([f\"Context {i+1}:\\n{t}\" for i, t in enumerate(top_texts)])\n",
        "    prompt = \"You are an assistant that answers HR policy questions based on the provided context.\\n\\n\"\n",
        "    prompt += f\"{context_text}\\n\\n\"\n",
        "    prompt += f\"Question: {question}\\nAnswer concisely:\"\n",
        "    # 3. generate an answer from the model\n",
        "    out = generator(prompt, max_new_tokens=max_length, do_sample=False)\n",
        "    return out[0][\"generated_text\"], scores\n",
        "\n",
        "# Quick local test\n",
        "q = \"When is payroll processed?\"\n",
        "ans, sims = answer_question(q, k=2)\n",
        "print(\"Similarity scores:\", sims)\n",
        "print(\"Answer:\\n\", ans)"
      ],
      "metadata": {
        "id": "af2znTQh73n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are important documents employee should submit while onboarding?\",\n",
        "    \"What is the sick leave rule?\",\n",
        "    \"Can I work from home more than 2 days?\",\n",
        "    \"When is payroll processed?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    ans, sims = answer_question(q, k=2)\n",
        "    print(\"Q:\", q)\n",
        "    print(\"Similarity:\", sims)\n",
        "    print(\"A:\", ans.strip(), \"\\n\" + \"-\"*60)"
      ],
      "metadata": {
        "id": "F5iud7Lr8zzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fastapi\n",
        "uvicorn[standard]\n",
        "gradio\n",
        "sentence-transformers\n",
        "transformers\n",
        "torch\n",
        "chromadb\n",
        "requests\n",
        "python-multipart"
      ],
      "metadata": {
        "id": "2p4AvLAdAaC6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "836a001e"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "fastapi\n",
        "uvicorn[standard]\n",
        "gradio\n",
        "sentence-transformers\n",
        "transformers\n",
        "torch\n",
        "chromadb\n",
        "requests\n",
        "python-multipart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "I-HV3Jeok7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ingest.py\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# 1. Load text\n",
        "loader = TextLoader(\"hr_policy.txt\", encoding=\"utf-8\")\n",
        "docs = loader.load()   # list of Document objects (page_content)\n",
        "\n",
        "# 2. Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "texts = [c.page_content for c in chunks]\n",
        "\n",
        "# 3. Embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# 4. Persist embeddings + texts (simple)\n",
        "os.makedirs(\"vector_data\", exist_ok=True)\n",
        "# Save texts\n",
        "with open(\"vector_data/texts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(texts, f)\n",
        "# Save embeddings (numpy)\n",
        "import numpy as np\n",
        "np.save(\"vector_data/embeddings.npy\", embeddings)\n",
        "\n",
        "print(\"Ingested:\", len(texts), \"chunks. Embeddings saved in vector_data/\")"
      ],
      "metadata": {
        "id": "x3FntkmHlIPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app/chat_backend.py\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from scipy.spatial.distance import cdist\n",
        "import pickle, os\n",
        "\n",
        "# Load persisted data\n",
        "# BASE = os.path.dirname(os.path.dirname(__file__))  # project root when run from app/\n",
        "BASE = \"/content\" # Assuming the data is saved in /content/vector_data\n",
        "texts_path = os.path.join(BASE, \"vector_data\", \"texts.pkl\")\n",
        "emb_path = os.path.join(BASE, \"vector_data\", \"embeddings.npy\")\n",
        "\n",
        "with open(texts_path, \"rb\") as f:\n",
        "    TEXTS = pickle.load(f)\n",
        "EMBEDDINGS = np.load(emb_path)\n",
        "\n",
        "# Load embedding model for queries (same model so vectors comparable)\n",
        "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load generator (Flan-T5)\n",
        "MODEL_NAME = \"google/flan-t5-small\"\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MODEL = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "GENERATOR = pipeline(\"text2text-generation\", model=MODEL, tokenizer=TOKENIZER, device=0 if __import__(\"torch\").cuda.is_available() else -1)\n",
        "\n",
        "def retrieve_top_k(question, k=2):\n",
        "    q_emb = EMBED_MODEL.encode([question], convert_to_numpy=True)  # (1, dim)\n",
        "    distances = cdist(q_emb, EMBEDDINGS, metric=\"cosine\")[0]       # (n_chunks,)\n",
        "    topk_idx = np.argsort(distances)[:k]\n",
        "    top_texts = [TEXTS[i] for i in topk_idx]\n",
        "    top_scores = [1 - float(distances[i]) for i in topk_idx]\n",
        "    return top_texts, top_scores\n",
        "\n",
        "def build_prompt(contexts, question):\n",
        "    # strict prompt: only use context\n",
        "    prompt = \"You are an assistant that answers questions using ONLY the provided context. If answer not present, say 'I don't know based on the provided documents.'\\n\\n\"\n",
        "    for i, c in enumerate(contexts, 1):\n",
        "        prompt += f\"[Context {i}]\\n{c}\\n\\n\"\n",
        "    prompt += f\"Question: {question}\\nAnswer concisely and cite the context like [Context 1].\"\n",
        "    return prompt\n",
        "\n",
        "def answer_question(question, k=2, max_length=256):\n",
        "    contexts, scores = retrieve_top_k(question, k=k)\n",
        "    prompt = build_prompt(contexts, question)\n",
        "    out = GENERATOR(prompt, max_length=max_length, do_sample=False)\n",
        "    answer = out[0][\"generated_text\"]\n",
        "    return {\"answer\": answer, \"sources\": [{\"text\": contexts[i], \"score\": scores[i], \"idx\": i} for i in range(len(contexts))]}"
      ],
      "metadata": {
        "id": "X6zU2KstllfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96d1d9d6"
      },
      "source": [
        "# Test the answer_question function\n",
        "question = \"What is the sick leave policy?\"\n",
        "response = answer_question(question)\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "print(\"Sources:\", response[\"sources\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app/main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "# from app.chat_backend import answer_question # Remove this line\n",
        "# from X6zU2KstllfW import answer_question # Import directly from the cell where it's defined\n",
        "\n",
        "\n",
        "app = FastAPI(title=\"HR RAG Chatbot\")\n",
        "\n",
        "class Query(BaseModel):\n",
        "    question: str\n",
        "    k: int = 2\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(q: Query):\n",
        "    res = answer_question(q.question, k=q.k)\n",
        "    return res\n",
        "\n",
        " # uvicorn app.main:app --reload --port 8000"
      ],
      "metadata": {
        "id": "NNCZF50ymYdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc02b01d",
        "outputId": "33d2dc14-b791-40fa-86fb-db0fe4ccb7db"
      },
      "source": [
        "# Run the FastAPI application with uvicorn\n",
        "# This will expose the application to the internet via ngrok (automatically handled by Colab)\n",
        "!uvicorn __main__:app --reload --port 8000"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m12033\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "bc89ccbc",
        "outputId": "7c791146-2f0f-4b7f-8c70-cfa4c124c795"
      },
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "# Define the FastAPI endpoint URL\n",
        "# In Colab, this will be the public URL provided by ngrok when you run FastAPI\n",
        "# You'll need to get this URL after running the uvicorn cell (bc02b01d)\n",
        "FASTAPI_URL = \"http://127.0.0.1:8000\" # Replace with your actual URL from the uvicorn output\n",
        "\n",
        "def get_answer_from_backend(question, k):\n",
        "    \"\"\"Sends a question to the FastAPI backend and returns the response.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(f\"{FASTAPI_URL}/chat\", json={\"question\": question, \"k\": k})\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        answer = data.get(\"answer\", \"No answer found.\")\n",
        "        sources = data.get(\"sources\", [])\n",
        "        source_text = \"\\n\\nSources:\\n\" + \"\\n\".join([f\"- Context {s['idx'] + 1}: {s['text']}\" for s in sources])\n",
        "        return answer + source_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error communicating with backend: {e}\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_answer_from_backend,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Ask a question about the HR Policy\"),\n",
        "        gr.Slider(minimum=1, maximum=5, value=2, step=1, label=\"Number of source chunks (k)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"HR Policy Chatbot\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(inline=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f0381cc9e9c6a78d45.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0381cc9e9c6a78d45.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca8a54cb"
      },
      "source": [
        "**Note:** After running the cell above, you will see a public URL generated by Gradio. You can use this URL to access your chatbot interface in a separate browser tab. Remember to replace `\"YOUR_FASTAPI_URL_HERE\"` in the code with the actual URL provided by ngrok when you run the FastAPI application in cell `bc02b01d`."
      ]
    }
  ]
}